Prerequisites:
1. Created a conda environment
2. Installed CMake (conda install -c conda-forge cmake -y)
3. Installed Visual Studio 2022 with C++ development tools
4. Installed cuda-toolkit (if using GPU) (conda install cuda-toolkit=12.6 cuda-nvcc=12.6 -c nvidia -y)
5. SentencePiece installed (conda install -c conda-forge sentencepiece -y)
6. Curl Installed (conda install -c conda-forge libcurl -y)
7. Copy cuda files to C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\MSBuild\Microsoft\VC\v170\BuildCustomizations
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6\extras\visual_studio_integration\MSBuildExtensions

### INSTRUCTIONS FOR SETTING UP LLAMA.CPP ON WINDOWS ###
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

mkdir build2

#CPU#
cd build2
cmake .. -G "Visual Studio 17 2022" -A x64
cmake --build . --config Release

#GPU#
cmake -B build-gpu -DGGML_CUDA=ON
# Specify Cuda Location
cmake -B build-gpu -DGGML_CUDA=ON -DCMAKE_GENERATOR_TOOLSET="cuda=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.6"
cmake --build build-gpu --config Release

cd ..

### CONVERTING HF MODEL TO GGUF FORMAT ###
run the python script to download the model (download_model.py)

cd llama.cpp

python convert_hf_to_gguf.py --outtype f16 --outfile ../ggufs/Ministral-8B-Instruct-2410.gguf ../models/Ministral-8B-Instruct-2410

### QUANTIZATION ###
cd build-gpu\bin\Release
.\llama-quantize.exe ..\..\..\..\ggufs\Mistral-7B-Instruct-v0.3.gguf ..\..\..\..\ggufs\Mistral-7B-Instruct-v0.3.Q4_K.gguf Q4_K
copy ..\..\..\..\ggufs\Mistral-7B-Instruct-v0.3.Q4_0.gguf C:\Users\Joel\Documents\Vscode\live_summary\models\

### Imatrix-QUANTIZATION ###
run the imatrix quantization script (imatrix.py) to download dataset
.\llama-imatrix -m ..\..\..\..\..\mistral-7b-instruct-v0.3.gguf -f ..\..\..\..\..\wiki.train.raw -o ..\..\..\..\..\mistral-7b-instruct-v0.3.imatrix.gguf --chunks 2000 --ctx-size 512 -ngl 80 
.\llama-quantize --imatrix ..\..\..\..\..\mistral-7b-instruct-v0.3.imatrix.gguf ..\..\..\..\..\mistral-7b-instruct-v0.3.gguf ..\..\..\..\..\mistral-7b-instruct-v0.3-Q4_K_M.gguf Q4_K_M

### QUANTIZATION FORMATS ###
Basic Quantization Formats
Q4_0
Q4_1
Q5_0
Q5_1
Q8_0
F16
F32

K-Quantization Formats
Q2_K
Q2_K_S
Q3_K_S
Q3_K_M
Q3_K_L
Q4_K_S
Q4_K_M
Q5_K_S
Q5_K_M
Q6_K:

I-Quantization Formats (Advanced)
IQ1_S: 1.56 bpw quantization
IQ1_M: 1.75 bpw quantization
IQ2_XXS: 2.06 bpw quantization
IQ2_XS: 2.31 bpw quantization
IQ2_S: 2.5 bpw quantization
IQ2_M: 2.7 bpw quantization
IQ3_XXS: 3.06 bpw quantization
IQ3_XS: 3.3 bpw quantization
IQ3_S: 3.44 bpw quantization
IQ3_M: 3.66 bpw quantization mix
IQ4_XS: 4.25 bpw non-linear quantization
IQ4_NL: 4.50 bpw non-linear quantization
Ternary Quantization
TQ1_0: 1.69 bpw ternarization
TQ2_0: 2.06 bpw ternarization
Special Format
COPY: Only copy tensors, no quantizing




VLM (Qwen2.5VL)
pip install torch torchvision transformers accelerate qwen-vl-utils
pip install git+https://github.com/huggingface/transformers accelerate
python download_model_VLM.py

git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
mkdir build2
cd build2
cmake .. -G "Visual Studio 17 2022" -A x64
cmake --build . --config Release

cd ..
# Convert the model to GGUF format (F16)
python convert_hf_to_gguf.py --outfile ../qwen2.5-vl-7b.gguf --outtype f16 ../Qwen2.5-VL-7B-Instruct
# Generate the multimodal projector
python convert_hf_to_gguf.py --outfile ../mmproj-qwen2.5-vl-7b.gguf --outtype f16 --mmproj ../Qwen2.5-VL-7B-Instruct

cd build2/bin/Release
./llama-quantize.exe ../../../../qwen2.5-vl-7b.gguf ../../../../qwen2.5-vl-7b-q4_k.gguf q4_k

# For single-turn query with an image
./build/bin/Release/llama-mtmd-cli.exe -m ./qwen2.5-vl-7b-q4_k.gguf --mmproj ./mmproj-qwen2.5-vl-7b.gguf --image path/to/your/image.jpg -p "What can you see in this image?"
# For interactive conversation mode
./build/bin/Release/llama-mtmd-cli.exe -m ./qwen2.5-vl-7b-q4_k.gguf --mmproj ./mmproj-qwen2.5-vl-7b.gguf -c 4096 --temp 0.7



python .\convert_hf_to_gguf.py --print-supported-models
Supported models:
TEXT models:
  - ArceeForCausalLM
  - ArcticForCausalLM
  - BaiChuanForCausalLM
  - BaichuanForCausalLM
  - BailingMoeForCausalLM
  - BambaForCausalLM
  - BertForMaskedLM
  - BertForSequenceClassification
  - BertModel
  - BitnetForCausalLM
  - BloomForCausalLM
  - BloomModel
  - CamembertModel
  - ChameleonForCausalLM
  - ChameleonForConditionalGeneration
  - ChatGLMForConditionalGeneration
  - ChatGLMModel
  - CodeShellForCausalLM
  - Cohere2ForCausalLM
  - CohereForCausalLM
  - DbrxForCausalLM
  - DeciLMForCausalLM
  - DeepseekForCausalLM
  - DeepseekV2ForCausalLM
  - DeepseekV3ForCausalLM
  - DistilBertForMaskedLM
  - DistilBertForSequenceClassification
  - DistilBertModel
  - Dots1ForCausalLM
  - DreamModel
  - Ernie4_5_ForCausalLM
  - Ernie4_5_MoeForCausalLM
  - Exaone4ForCausalLM
  - ExaoneForCausalLM
  - FalconForCausalLM
  - FalconH1ForCausalLM
  - FalconMambaForCausalLM
  - GPT2LMHeadModel
  - GPTBigCodeForCausalLM
  - GPTNeoXForCausalLM
  - GPTRefactForCausalLM
  - Gemma2ForCausalLM
  - Gemma3ForCausalLM
  - Gemma3ForConditionalGeneration
  - Gemma3nForConditionalGeneration
  - GemmaForCausalLM
  - Glm4ForCausalLM
  - GlmForCausalLM
  - GraniteForCausalLM
  - GraniteMoeForCausalLM
  - GraniteMoeHybridForCausalLM
  - GraniteMoeSharedForCausalLM
  - GrokForCausalLM
  - HunYuanMoEV1ForCausalLM
  - InternLM2ForCausalLM
  - InternLM3ForCausalLM
  - JAISLMHeadModel
  - JambaForCausalLM
  - JinaBertForMaskedLM
  - JinaBertModel
  - LFM2ForCausalLM
  - LLaMAForCausalLM
  - Lfm2ForCausalLM
  - Llama4ForConditionalGeneration
  - LlamaForCausalLM
  - LlamaModel
  - LlavaForConditionalGeneration
  - LlavaStableLMEpochForCausalLM
  - MPTForCausalLM
  - MT5ForConditionalGeneration
  - Mamba2ForCausalLM
  - MambaForCausalLM
  - MambaLMHeadModel
  - MiniCPM3ForCausalLM
  - MiniCPMForCausalLM
  - Mistral3ForConditionalGeneration
  - MistralForCausalLM
  - MixtralForCausalLM
  - NemotronForCausalLM
  - NeoBERT
  - NeoBERTForSequenceClassification
  - NeoBERTLMHead
  - NomicBertModel
  - OLMoForCausalLM
  - Olmo2ForCausalLM
  - OlmoForCausalLM
  - OlmoeForCausalLM
  - OpenELMForCausalLM
  - OrionForCausalLM
  - PLMForCausalLM
  - PLaMo2ForCausalLM
  - Phi3ForCausalLM
  - PhiForCausalLM
  - PhiMoEForCausalLM
  - Plamo2ForCausalLM
  - PlamoForCausalLM
  - QWenLMHeadModel
  - Qwen2AudioForConditionalGeneration
  - Qwen2ForCausalLM
  - Qwen2Model
  - Qwen2MoeForCausalLM
  - Qwen2VLForConditionalGeneration
  - Qwen2VLModel
  - Qwen2_5OmniModel
  - Qwen2_5_VLForConditionalGeneration
  - Qwen3ForCausalLM
  - Qwen3MoeForCausalLM
  - RWForCausalLM
  - RWKV6Qwen2ForCausalLM
  - RWKV7ForCausalLM
  - RobertaForSequenceClassification
  - RobertaModel
  - Rwkv6ForCausalLM
  - Rwkv7ForCausalLM
  - RwkvHybridForCausalLM
  - SmolLM3ForCausalLM
  - StableLMEpochForCausalLM
  - StableLmForCausalLM
  - Starcoder2ForCausalLM
  - T5EncoderModel
  - T5ForConditionalGeneration
  - T5WithLMHeadModel
  - UMT5ForConditionalGeneration
  - UltravoxModel
  - VLlama3ForCausalLM
  - WavTokenizerDec
  - XLMRobertaForSequenceClassification
  - XLMRobertaModel
  - XverseForCausalLM
MMPROJ models:
  - Gemma3ForConditionalGeneration
  - Idefics3ForConditionalGeneration
  - InternVisionModel
  - Llama4ForConditionalGeneration
  - LlavaForConditionalGeneration
  - Mistral3ForConditionalGeneration
  - Qwen2AudioForConditionalGeneration
  - Qwen2VLForConditionalGeneration
  - Qwen2VLModel
  - Qwen2_5OmniModel
  - Qwen2_5_VLForConditionalGeneration
  - SmolVLMForConditionalGeneration
  - UltravoxModel